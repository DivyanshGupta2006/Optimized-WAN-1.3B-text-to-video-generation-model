{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and upgrade all dependencies\n",
    "!pip install --upgrade diffusers transformers ftfy accelerate imageio lpips torch torchvision decord git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import cv2\n",
    "import lpips\n",
    "from torchvision import transforms\n",
    "import imageio\n",
    "import gradio as gr\n",
    "import torch\n",
    "import subprocess\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables for accessing video and frames\n",
    "video_path = \"output_video.mp4\"\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\n",
    "output_frames_path = \"output_frames.mp4\"\n",
    "denoise_steps = [0]*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tqdm = tqdm.auto.tqdm\n",
    "class TqdmSpy(original_tqdm):\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        global denoise_steps\n",
    "        denoise_steps[self.n-1] = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "diffusers.utils.tqdm = TqdmSpy\n",
    "tqdm.tqdm = TqdmSpy\n",
    "tqdm.auto.tqdm = TqdmSpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "from decord import VideoReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model for CLIP Score\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPIPS loss function and transform definition\n",
    "loss_fn = lpips.LPIPS(net='vgg')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model while recording time to load the model\n",
    "start_time = time.time()\n",
    "pipe = diffusers.DiffusionPipeline.from_pretrained(model_id, device_map=\"balanced\")\n",
    "end_time = time.time()\n",
    "load_time=end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for encoding prompt\n",
    "def patch_encode_prompt(pipe):\n",
    "    text_encoder_device = pipe.text_encoder.get_input_embeddings().weight.device\n",
    "    orig_encode_prompt = pipe.encode_prompt\n",
    "\n",
    "    def patched_encode_prompt(*args, **kwargs):\n",
    "        if \"device\" not in kwargs or kwargs[\"device\"] is None:\n",
    "            kwargs[\"device\"] = text_encoder_device\n",
    "        return orig_encode_prompt(*args, **kwargs)\n",
    "\n",
    "    pipe.encode_prompt = patched_encode_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for extracting frames from video for CLIP score\n",
    "def extract_frames(num_frames=8):\n",
    "    vr = VideoReader(video_path)\n",
    "    total_frames = len(vr)\n",
    "    indices = torch.linspace(0, total_frames - 1, steps=num_frames).long()\n",
    "    batch = vr.get_batch(indices).asnumpy()  # shape: (T, H, W, C)\n",
    "\n",
    "    return [Image.fromarray(frame) for frame in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for extracting frames from video for LPIPS score\n",
    "def extract_frames_imageio(video_path):\n",
    "    reader = imageio.get_reader(video_path)\n",
    "    frames = []\n",
    "    for frame in reader:\n",
    "        frames.append(Image.fromarray(frame))\n",
    "    reader.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to plot graph of denoising\n",
    "def denoise_graph():\n",
    "    global denoise_steps\n",
    "    dns2 = [denoise_steps[i] - denoise_steps[i-1] for i in range(1, len(denoise_steps))]\n",
    "    plt.plot(list(range(len(dns2))), dns2)\n",
    "    plt.xlabel('Denoising step')\n",
    "    plt.ylabel('Time taken')\n",
    "    plt.title('Denoising')\n",
    "    graph = plt.gcf()\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for computing CLIP score\n",
    "def compute_clip_score(frames, text):\n",
    "    text_token = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_token).float()\n",
    "        frame_features = []\n",
    "        for frame in frames:\n",
    "            image_input = preprocess(frame).unsqueeze(0).to(device)\n",
    "            image_feature = model.encode_image(image_input).float()\n",
    "            frame_features.append(image_feature)\n",
    "        frame_features = torch.stack(frame_features).squeeze(1)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        frame_features = frame_features / frame_features.norm(dim=-1, keepdim=True)\n",
    "        similarities = (frame_features @ text_features.T).squeeze()\n",
    "        return similarities.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for computing LPIPS score\n",
    "def compute_temporal_lpips(frames):\n",
    "    if not frames or len(frames) < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(frames) - 1):\n",
    "        try:\n",
    "            img1 = transform(frames[i]).unsqueeze(0)\n",
    "            img2 = transform(frames[i + 1]).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dist = loss_fn(img1, img2)\n",
    "                score = dist.item()\n",
    "                scores.append(score)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if not scores:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get model load time\n",
    "def data_load_time():\n",
    "  return f\"{load_time:.3f} s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to get GPU stats\n",
    "def get_gpu_stats():\n",
    "    try:\n",
    "        result = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=memory.used,memory.free,utilization.gpu,temperature.gpu,power.draw\",\n",
    "            \"--format=csv,nounits,noheader\"\n",
    "        ], encoding='utf-8')\n",
    "        memory_used, memory_free, utilization, temp, power = result.strip().split(', ')\n",
    "        return {\n",
    "            \"memory_used\": f\"{memory_used} MB\",\n",
    "            \"memory_free\": f\"{memory_free} MB\",\n",
    "            \"utilization\": f\"{utilization} %\",\n",
    "            \"temperature\": f\"{temp} Â°C\",\n",
    "            \"power\": f\"{power} W\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"memory_used\": \"Error\",\n",
    "            \"memory_free\": \"Error\",\n",
    "            \"utilization\": \"Error\",\n",
    "            \"temperature\": \"Error\",\n",
    "            \"power\": \"Error\"\n",
    "        }\n",
    "def get_gpu_info_only():\n",
    "    stats = get_gpu_stats()\n",
    "    return (\n",
    "        stats[\"memory_used\"],\n",
    "        stats[\"memory_free\"],\n",
    "        stats[\"utilization\"],\n",
    "        stats[\"temperature\"],\n",
    "        stats[\"power\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video(prompt, negative_prompt=\"Blurry, unrealistic, shaky\", frames=60, fps=12, resolution=480):\n",
    "\n",
    "\n",
    "    global denoise_steps\n",
    "    denoise_steps = [0] * 50\n",
    "\n",
    "    # Adjust frame count as per WAN's requirement\n",
    "    frames = 4 * frames + 1\n",
    "\n",
    "    # Set height and width based on resolution\n",
    "    height = resolution\n",
    "    width = 832  # default\n",
    "\n",
    "    if height == 240:\n",
    "        width = 416\n",
    "    elif height == 720:\n",
    "        width = 1248\n",
    "    elif height == 1080:\n",
    "        width = 1872\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "    # Measure generation time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate video frames\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_frames=frames,\n",
    "        guidance_scale=5.0,\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_latency = end_time - start_time\n",
    "    frame_latency = total_latency / frames\n",
    "    Throughput = frames / total_latency\n",
    "\n",
    "    video = output.frames\n",
    "\n",
    "    if isinstance(video, np.ndarray):\n",
    "        video = np.squeeze(video)\n",
    "        video = (video * 255).clip(0, 255).astype(\"uint8\")\n",
    "    else:\n",
    "        raise TypeError(\"Unexpected output format from pipeline\")\n",
    "\n",
    "    # Convert frames to PIL Images\n",
    "    frame_images = [Image.fromarray(frame) for frame in video]\n",
    "\n",
    "    # Export videos\n",
    "    diffusers.utils.export_to_video(frame_images, video_path, fps=fps)\n",
    "    diffusers.utils.export_to_video(frame_images, output_frames_path, fps=1)\n",
    "\n",
    "    framess = extract_frames(num_frames=frames)\n",
    "    score = compute_clip_score(framess, prompt)\n",
    "\n",
    "\n",
    "    return video_path, f\"{total_latency:.3f} s\", f\"{frame_latency:.3f} s\", f\"{score:.3f}\", f\"{Throughput:.3f} fps\", denoise_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_encode_prompt(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themeeeeee = gr.themes.Base(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"gray\",\n",
    "    radius_size=gr.themes.Size(\n",
    "        xxs=\"6px\", xs=\"6px\", sm=\"8px\", md=\"10px\", lg=\"12px\", xl=\"14px\", xxl=\"16px\"\n",
    "    ),\n",
    "    spacing_size=gr.themes.Size(\n",
    "        xxs=\"2px\", xs=\"4px\", sm=\"6px\", md=\"10px\", lg=\"16px\", xl=\"24px\", xxl=\"32px\"\n",
    "    )\n",
    ").set(\n",
    "    body_background_fill=\"linear-gradient(135deg, #0f2027, #203a43, #2c5364)\",\n",
    "    body_text_color=\"white\",\n",
    "    block_background_fill=\"rgba(255, 255, 255, 0.08)\",\n",
    "    block_border_color=\"rgba(255, 255, 255, 0.2)\",\n",
    "    block_shadow=\"0 12px 40px rgba(0, 0, 0, 0.4)\",\n",
    "    input_background_fill=\"rgba(255, 255, 255, 0.1)\",\n",
    "    input_border_color=\"rgba(255, 255, 255, 0.2)\",\n",
    "    button_primary_background_fill=\"rgba(99, 102, 241, 0.85)\",\n",
    "    button_primary_text_color=\"white\",\n",
    "    button_primary_background_fill_hover=\"rgba(99, 102, 241, 1)\"\n",
    ")\n",
    "\n",
    "css_reset = \"\"\"\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap');\n",
    "body, #root, .gradio-container {\n",
    "    font-family: 'Inter', sans-serif !important;\n",
    "    background: linear-gradient(135deg, #0f2027, #203a43, #2c5364);\n",
    "    perspective: 1000px;\n",
    "    overflow-x: hidden;\n",
    "    animation: fadeIn 1s ease-in-out;\n",
    "}\n",
    "#root { transform-style: preserve-3d; }\n",
    ".gradio-container > * {\n",
    "    /*transform: rotateX(1deg) rotateY(-2deg);*/\n",
    "    transition: transform 0.4s ease, box-shadow 0.4s ease;\n",
    "    backdrop-filter: blur(14px);\n",
    "    -webkit-backdrop-filter: blur(14px);\n",
    "    border-radius: 20px;\n",
    "    background: rgba(255, 255, 255, 0.05);\n",
    "    box-shadow: 0 12px 40px rgba(0, 0, 0, 0.3);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.15);\n",
    "    margin: 30px auto;\n",
    "    padding: 25px;\n",
    "    width: 85% !important;\n",
    "    box-sizing: border-box;\n",
    "    color: white !important;\n",
    "}\n",
    ".gr-button { font-weight: bold; border-radius: 12px !important; box-shadow: 0 4px 16px rgba(0,0,0,0.3); }\n",
    ".gr-button:hover { transform: scale(1.05); }\n",
    ".gr-slider input[type=\"range\"] { accent-color: #6366f1; }\n",
    "@keyframes fadeIn { from { opacity: 0; transform: translateY(20px); } to { opacity: 1; transform: translateY(0); } }\n",
    "\n",
    "/* Accordion container */\n",
    ".gr-accordion {\n",
    "    background-color: #00ffff !important;   /* Cyan background */\n",
    "    color: #ffffff !important;             /* White text */\n",
    "    border-radius: 8px;\n",
    "    border: 1px solid #444;\n",
    "    padding: 4px;\n",
    "}\n",
    "\n",
    "/* Accordion header */\n",
    ".gr-accordion .prose {\n",
    "    color: #1e1e2f !important;             /* Dark heading text */\n",
    "    font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(theme=themeeeeee) as demo:\n",
    "    gr.HTML(css_reset)\n",
    "    gr.Markdown(\"# Accelerate-WAN\")\n",
    "    gr.Markdown(\"<hr>\")\n",
    "\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\"## <div style='text-align:center; padding:15px;'>Metrics</div>\")\n",
    "        with gr.Row():\n",
    "            with gr.Accordion(\"Hardware\", open=False):\n",
    "                with gr.Row():\n",
    "                    mem_used = gr.Textbox(label=\"Memory used\", interactive=False)\n",
    "                    mem_free = gr.Textbox(label=\"Memory free\", interactive=False)\n",
    "                    gpu_util = gr.Textbox(label=\"GPU utilization\", interactive=False)\n",
    "                    temp = gr.Textbox(label=\"Temperature\", interactive=False)\n",
    "                    powe = gr.Textbox(label=\"Power draw\", interactive=False)\n",
    "            with gr.Accordion(\"Efficiency\", open=True):\n",
    "                with gr.Row():\n",
    "                    clip_latency_box = gr.Textbox(label=\"Clip-wise latency\", interactive=False)\n",
    "                    frame_latency_box = gr.Textbox(label=\"Frame-wise latency\", interactive=False)\n",
    "                    throughput_box = gr.Textbox(label=\"Throughput\", interactive=False)\n",
    "                    dngraphoutput = gr.Plot(label=\"Time vs Denoising-Steps Graph\")\n",
    "        with gr.Row():\n",
    "            with gr.Accordion(\"Accuracy\", open=False):\n",
    "                with gr.Row():\n",
    "                    lpips_score_box = gr.Textbox(label=\"LPIPS score\", interactive=False)\n",
    "                    clip_score_box = gr.Textbox(label=\"CLIP score\", interactive=False)\n",
    "            with gr.Accordion(\"Others\", open=False):\n",
    "                with gr.Row():\n",
    "                    gr.Textbox(label=\"Compile time\", interactive=False)\n",
    "                    lolu = gr.Textbox(label=\"Load time\", interactive=False)\n",
    "                    gr.Textbox(label=\"Batch processing efficiency\", interactive=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Group():\n",
    "            gr.Markdown(\"## <div style='text-align:center; padding:15px;'>Parameters</div>\")\n",
    "            res = gr.Radio(choices=[240, 480, 720, 1080], value=480, label=\"Output resolution\", interactive=True)\n",
    "            with gr.Row():\n",
    "                prompt = gr.Textbox(placeholder=\"e.g. A cat walking on moon\", label=\"Prompt\")\n",
    "                nprompt = gr.Textbox(value = negative_prompt, label=\"Negative prompt\")\n",
    "            with gr.Row():\n",
    "                fps = gr.Slider(minimum=1, maximum=120, label=\"FPS\", interactive=True, value=12)\n",
    "                frames = gr.Slider(minimum=1, maximum=480, label=\"Number of frames\", interactive=True, value=61)\n",
    "            opt = gr.CheckboxGroup(choices = ['Flash Attention', 'Operator Fusion', 'CFG Parallelism', 'LoRA', 'Quantization', 'Best'], value = 'Best', label = 'Optimization techniques')\n",
    "\n",
    "        with gr.Group():\n",
    "            gr.Markdown(\"## <div style='text-align:center; padding:15px;'>Output</div>\")\n",
    "            output = gr.Video(label=\"Generated video\")\n",
    "\n",
    "    generate = gr.Button(\"Generate\")\n",
    "\n",
    "    # Timers for GPU and load time monitoring\n",
    "    timer = gr.Timer()\n",
    "    timer.tick(fn=get_gpu_info_only, inputs=[], outputs=[mem_used, mem_free, gpu_util, temp, powe])\n",
    "    timer.tick(fn=data_load_time, inputs=[], outputs=[lolu])\n",
    "\n",
    "    # Wrapper with LPIPS integration\n",
    "    def wrapper(prompt, nprompt, frames, fps, res):\n",
    "        video_path, clip_latency, frame_latency, clip_scoree, throughputt, dngraph = generate_video(prompt, nprompt, frames, fps, res)\n",
    "        extracted_frames = extract_frames_imageio(video_path)\n",
    "        lpips_score = compute_temporal_lpips(extracted_frames)\n",
    "        lpips_display = f\"{lpips_score:.3f}\" if not np.isnan(lpips_score) else \"N/A\"\n",
    "        return video_path, clip_latency, frame_latency, lpips_display, clip_scoree, throughputt, dngraph\n",
    "\n",
    "    generate.click(\n",
    "        fn=wrapper,\n",
    "        inputs=[prompt, nprompt, frames, fps, res],\n",
    "        outputs=[output, clip_latency_box, frame_latency_box, lpips_score_box, clip_score_box, throughput_box, dngraphoutput]\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
