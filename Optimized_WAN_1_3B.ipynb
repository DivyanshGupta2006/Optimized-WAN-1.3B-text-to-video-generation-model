{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import lpips\n",
    "from torchvision import transforms\n",
    "import imageio\n",
    "import gradio as gr\n",
    "import torch\n",
    "import subprocess\n",
    "import clip\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables for accessing video and frames\n",
    "video_path = \"output_video.mp4\"\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\n",
    "output_frames_path = \"output_frames.mp4\"\n",
    "denoise_steps = [0] * 50\n",
    "load_time = 0\n",
    "warm_up_time = 0\n",
    "denoise_time = 0\n",
    "is_LoRA_applied = False\n",
    "is_operator_fusion_applied = False\n",
    "is_quantization_applied = False\n",
    "is_CPU_offload_applied = False\n",
    "pipe = None\n",
    "model = None\n",
    "preprocess = None\n",
    "model_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the tqdm library to note time at each inference step\n",
    "original_tqdm = tqdm.auto.tqdm\n",
    "class TqdmSpy(original_tqdm):\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        global denoise_steps\n",
    "        denoise_steps[self.n-1] = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch modified tqdm to diffusers\n",
    "import diffusers\n",
    "diffusers.utils.tqdm = TqdmSpy\n",
    "tqdm.tqdm = TqdmSpy\n",
    "tqdm.auto.tqdm = TqdmSpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for extracting frames\n",
    "import decord\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "from decord import VideoReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA H100 80GB HBM3')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check device availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/zeus/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528M/528M [00:02<00:00, 228MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "# LPIPS loss function and transform definition\n",
    "loss_fn = lpips.LPIPS(net='vgg')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to set schedulers\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "def set_diffusion_scheduler(pipe, scheduler_type):\n",
    "    config = pipe.scheduler.config\n",
    "\n",
    "    if scheduler_type == \"dpm\":\n",
    "        pipe.scheduler = DPMSolverMultistepScheduler.from_config(config)\n",
    "        print(\"[INFO] Scheduler set to DPM Solver Multistep\")\n",
    "    else:\n",
    "        print(\"[INFO] Using default scheduler (no change)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load model while recording time to load the model\n",
    "def load_model(data_type, pipeline_quant_config, scheduler_type, to_offload=True):\n",
    "    global load_time\n",
    "    global pipe\n",
    "    global model\n",
    "    global preprocess\n",
    "    pipe = None\n",
    "    model = None\n",
    "    preprocess = None\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    map = None\n",
    "\n",
    "    if to_offload:\n",
    "        map = \"balanced\"\n",
    "\n",
    "    print(f\"Applying CPU offloading...Choosing device-map: {map}\")\n",
    "\n",
    "    pipe = diffusers.DiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=map,\n",
    "        torch_dtype=data_type,\n",
    "        quantization_config=pipeline_quant_config\n",
    "    )\n",
    "\n",
    "    if (not to_offload):\n",
    "        pipe.to(device)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # load model for CLIP Score\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    # Set scheduler\n",
    "    set_diffusion_scheduler(pipe, scheduler_type=scheduler_type)\n",
    "\n",
    "    patch_encode_prompt(pipe)\n",
    "\n",
    "    load_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for encoding prompt\n",
    "def patch_encode_prompt(pipe):\n",
    "    text_encoder_device = pipe.text_encoder.get_input_embeddings().weight.device\n",
    "    orig_encode_prompt = pipe.encode_prompt\n",
    "\n",
    "    def patched_encode_prompt(*args, **kwargs):\n",
    "        if \"device\" not in kwargs or kwargs[\"device\"] is None:\n",
    "            kwargs[\"device\"] = text_encoder_device\n",
    "        return orig_encode_prompt(*args, **kwargs)\n",
    "\n",
    "    pipe.encode_prompt = patched_encode_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for extracting frames from video for CLIP score\n",
    "def extract_frames(num_frames=8):\n",
    "    vr = VideoReader(video_path)\n",
    "    total_frames = len(vr)\n",
    "    indices = torch.linspace(0, total_frames - 1, steps=num_frames).long()\n",
    "    batch = vr.get_batch(indices).asnumpy()  # shape: (T, H, W, C)\n",
    "\n",
    "    return [Image.fromarray(frame) for frame in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for extracting frames from video for LPIPS score\n",
    "def extract_frames_imageio(video_path):\n",
    "    reader = imageio.get_reader(video_path)\n",
    "    frames = []\n",
    "    for frame in reader:\n",
    "        frames.append(Image.fromarray(frame))\n",
    "    reader.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to plot graph of denoising\n",
    "def denoise_graph():\n",
    "    global denoise_steps\n",
    "    global warm_up_time\n",
    "    global denoise_time\n",
    "    dns2 = [denoise_steps[i] - denoise_steps[i-1] for i in range(1, len(denoise_steps))]\n",
    "    denoise_time = sum(dns2)\n",
    "    plt.plot(list(range(len(dns2))), dns2)\n",
    "    plt.xlabel('Denoising step')\n",
    "    plt.ylabel('Time taken')\n",
    "    plt.title('Denoising')\n",
    "    graph = plt.gcf()\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for computing CLIP score\n",
    "def compute_clip_score(frames, text):\n",
    "    text_token = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_token).float()\n",
    "        frame_features = []\n",
    "        for frame in frames:\n",
    "            image_input = preprocess(frame).unsqueeze(0).to(device)\n",
    "            image_feature = model.encode_image(image_input).float()\n",
    "            frame_features.append(image_feature)\n",
    "        frame_features = torch.stack(frame_features).squeeze(1)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        frame_features = frame_features / frame_features.norm(dim=-1, keepdim=True)\n",
    "        similarities = (frame_features @ text_features.T).squeeze()\n",
    "        return similarities.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for computing LPIPS score\n",
    "def compute_temporal_lpips(frames):\n",
    "    if not frames or len(frames) < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(frames) - 1):\n",
    "        try:\n",
    "            img1 = transform(frames[i]).unsqueeze(0)\n",
    "            img2 = transform(frames[i + 1]).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dist = loss_fn(img1, img2)\n",
    "                score = dist.item()\n",
    "                scores.append(score)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if not scores:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to get GPU stats\n",
    "def get_gpu_stats():\n",
    "    try:\n",
    "        result = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=memory.used,memory.free,utilization.gpu,temperature.gpu,power.draw\",\n",
    "            \"--format=csv,nounits,noheader\"\n",
    "        ], encoding='utf-8')\n",
    "        memory_used, memory_free, utilization, temp, power = result.strip().split(', ')\n",
    "        return {\n",
    "            \"memory_used\": f\"{memory_used} MB\",\n",
    "            \"memory_free\": f\"{memory_free} MB\",\n",
    "            \"utilization\": f\"{utilization} %\",\n",
    "            \"temperature\": f\"{temp} °C\",\n",
    "            \"power\": f\"{power} W\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"memory_used\": \"Error\",\n",
    "            \"memory_free\": \"Error\",\n",
    "            \"utilization\": \"Error\",\n",
    "            \"temperature\": \"Error\",\n",
    "            \"power\": \"Error\"\n",
    "        }\n",
    "def get_gpu_info_only():\n",
    "    stats = get_gpu_stats()\n",
    "    return (\n",
    "        stats[\"memory_used\"],\n",
    "        stats[\"memory_free\"],\n",
    "        stats[\"utilization\"],\n",
    "        stats[\"temperature\"],\n",
    "        stats[\"power\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot GPU VRAM usage\n",
    "gpu_times=[]\n",
    "gpu_vram_usage=[]\n",
    "\n",
    "def only_vram():\n",
    "  statss = get_gpu_stats()\n",
    "  return(statss[\"memory_used\"])\n",
    "\n",
    "# Event to signal when to stop monitoring\n",
    "stop_event = threading.Event()\n",
    "stop_event.clear()\n",
    "\n",
    "def monitor_vram():\n",
    "    start = time.time()\n",
    "    while not stop_event.is_set():\n",
    "        mem = only_vram()\n",
    "        t = time.time() - start\n",
    "        if mem != \"Error\":\n",
    "            v = int(mem.split()[0])\n",
    "        else:\n",
    "            v = 0\n",
    "        gpu_times.append(t)\n",
    "        gpu_vram_usage.append(v)\n",
    "        time.sleep(0.25)\n",
    "def gpu_usage_graph():\n",
    "    global gpu_times\n",
    "    global gpu_vram_usage\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(gpu_times, gpu_vram_usage, marker='o', color='blue')\n",
    "    plt.xlabel('Time Elapsed (seconds)')\n",
    "    plt.ylabel('VRAM Used (MB)')\n",
    "    plt.title('GPU VRAM Usage During Video Generation')\n",
    "    plt.grid(True)\n",
    "    graph = plt.gcf()\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to implement attention slicing\n",
    "def apply_attention_slicing():\n",
    "    global pipe\n",
    "    if hasattr(pipe, \"enable_attention_slicing\"):\n",
    "        try:\n",
    "            pipe.enable_attention_slicing()\n",
    "            print(\"Attention slicing enabled.\")\n",
    "        except Exception as e:\n",
    "            print(\"Attention slicing failed:\", e)\n",
    "    else:\n",
    "        print(\"Attention slicing not supported on this pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to implement LoRA - loads weight from the git repo\n",
    "def apply_LoRA():\n",
    "    global is_LoRA_applied\n",
    "    global pipe\n",
    "    if (is_LoRA_applied == False):\n",
    "      lora_path = \"./Wan2.1-T2V-1.3B-crush-smol-v0/\"\n",
    "      print(\"Starting LoRA application...\")\n",
    "\n",
    "      # Check if the path exists\n",
    "      if not os.path.exists(lora_path):\n",
    "          print(f\"LoRA path does not exist: {lora_path}\")\n",
    "          return\n",
    "\n",
    "      try:\n",
    "          print(f\"Loading LoRA weights from: {lora_path}\")\n",
    "          pipe.load_lora_weights(lora_path)\n",
    "          print(\"LoRA weights loaded successfully.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Error loading LoRA weights: {e}\")\n",
    "          return\n",
    "\n",
    "      try:\n",
    "          print(\"Enabling LoRA...\")\n",
    "          pipe.enable_lora()\n",
    "          print(\"LoRA enabled.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Error enabling LoRA: {e}\")\n",
    "          return\n",
    "\n",
    "      try:\n",
    "          print(\"Attempting to fuse LoRA...\")\n",
    "          pipe.fuse_lora()\n",
    "          is_LoRA_applied = True\n",
    "          print(\"LoRA fused successfully.\")\n",
    "      except Exception as e:\n",
    "          print(f\"LoRA fuse not supported: {e}\")\n",
    "    else:\n",
    "      print(\"LoRA already applied, so skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to implement operator fusion - covers model transformer and VAE\n",
    "def apply_operator_fusion():\n",
    "    global is_operator_fusion_applied\n",
    "    global pipe\n",
    "    if (is_operator_fusion_applied == False):\n",
    "      print(\"Starting operator fusion...\")\n",
    "\n",
    "      try:\n",
    "          print(\"Compiling transformer...\")\n",
    "          pipe.transformer = torch.compile(pipe.transformer, mode=\"reduce-overhead\")\n",
    "          print(\"Transformer compiled successfully.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Error compiling transformer: {e}\")\n",
    "\n",
    "      try:\n",
    "          print(\"Compiling VAE...\")\n",
    "          pipe.vae = torch.compile(pipe.vae, mode=\"reduce-overhead\")\n",
    "          print(\"VAE compiled successfully.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Error compiling VAE: {e}\")\n",
    "\n",
    "      is_operator_fusion_applied = True\n",
    "    else:\n",
    "      print(\"Operator Fusion already applied, so skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to apply quantization - 4 bit\n",
    "def apply_quantization():\n",
    "    print(\"Applying quantization configuration...\")\n",
    "\n",
    "    dtype = torch.float16\n",
    "    print(f\"Chosen dtype: {dtype}\")\n",
    "\n",
    "    quant_config = diffusers.quantizers.PipelineQuantizationConfig(\n",
    "        quant_backend=\"bitsandbytes_4bit\",\n",
    "        quant_kwargs={\n",
    "            \"load_in_4bit\": True,\n",
    "            \"bnb_4bit_quant_type\": \"nf4\",\n",
    "            \"bnb_4bit_compute_dtype\": torch.float16\n",
    "        },\n",
    "        components_to_quantize=[\"transformer\", \"text_encoder\", \"vae\"]\n",
    "    )\n",
    "\n",
    "    print(\"Quantization config created.\")\n",
    "    print(f\"Quant backend: {quant_config.quant_backend}\")\n",
    "    print(f\"Quant kwargs: {quant_config.quant_kwargs}\")\n",
    "    print(f\"Components to quantize: {quant_config.components_to_quantize}\")\n",
    "\n",
    "    return dtype, quant_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video(prompt, negative_prompt=\"Blurry, unrealistic, shaky\", frames=60, fps=12, resolution=480, inference_steps=50, guidance_scale=6.0, seed=42):\n",
    "\n",
    "    global denoise_steps\n",
    "    global warm_up_time\n",
    "    global denoise_time\n",
    "    global pipe\n",
    "    \n",
    "    denoise_steps = [0] * inference_steps\n",
    "\n",
    "    # adjust frame count as per WAN's requirement\n",
    "    frames = 4 * frames + 1\n",
    "\n",
    "    # set height and width based on resolution\n",
    "    height = resolution\n",
    "    width = 832  # default\n",
    "\n",
    "    if height == 240:\n",
    "        width = 416\n",
    "    elif height == 720:\n",
    "        width = 1248\n",
    "\n",
    "    # set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    # measure generation time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # generate video frames\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_frames=frames,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "        num_inference_steps=inference_steps\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    grph = denoise_graph()\n",
    "    total_latency = end_time - start_time\n",
    "    warm_up_time = total_latency - denoise_time\n",
    "    frame_latency = denoise_time / frames\n",
    "    Throughput = frames / total_latency\n",
    "\n",
    "    video = output.frames\n",
    "\n",
    "    if isinstance(video, np.ndarray):\n",
    "        video = np.squeeze(video)\n",
    "        video = (video * 255).clip(0, 255).astype(\"uint8\")\n",
    "    else:\n",
    "        raise TypeError(\"Unexpected output format from pipeline\")\n",
    "\n",
    "    # convert frames to PIL Images\n",
    "    frame_images = [Image.fromarray(frame) for frame in video]\n",
    "\n",
    "    # export videos\n",
    "    diffusers.utils.export_to_video(frame_images, video_path, fps=fps)\n",
    "    diffusers.utils.export_to_video(frame_images, output_frames_path, fps=1)\n",
    "\n",
    "    return video_path, f\"{total_latency:.3f} s\", f\"{frame_latency:.3f} s\", f\"{Throughput:.3f} fps\", grph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://6d33f0eeb8c9428887.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6d33f0eeb8c9428887.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization mode: Best\n",
      "Applying quantization configuration...\n",
      "Chosen dtype: torch.float16\n",
      "Quantization config created.\n",
      "Quant backend: bitsandbytes_4bit\n",
      "Quant kwargs: {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': torch.float16}\n",
      "Components to quantize: ['transformer', 'text_encoder', 'vae']\n",
      "Applying CPU offloading...Choosing device-map: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e532ff5c2149e38ff85df04b6f38c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/400 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858fd58146464f75a7dad717231f798c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2838d691450040f2945309fd869a32b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/751 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e85345fb5b469ebd46f532a0a28ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d142b92d759c414b9b837db47f917776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a67289604049219fb6e47dbf564596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/2.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d116b965658a4ee496f7936c5adc6c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256f221388384f54998cabcd1c0a3c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092111968f7543319df047c4e2d860ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221debc3ba534a479817df3337fbf2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a90f41f8864475b392873cb673588e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f553477c64014c21ab6713a5efc0628f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6caa6fdd66461fb8bb1b2d933f2608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e69295423004b97867ad3db4c19ca83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93a2dbfeae44f5bbe69c0f1c3dca732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b41c3fd03a3426fb8b6f4ba40f89cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be04a4bde5847d9bf3c6fdec15b6522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)pytorch_model-00002-of-00002.safetensors:   0%|          | 0.00/677M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4769c5f835477bae821446c13d3d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ion_pytorch_model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f028a04f253e4479a7d764761df2d437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86faf076e94b4ff9afe08aca9bc47a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/508M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a765652edc504ea6a66e6a0a5e076994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a527bb4e5074ae99a9d11887ad0840c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90dd9913ebcf40e5935ba688d2b2ba8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are loading your model in 8bit or 4bit but no linear modules were found in your model. Please double check your model architecture, or submit an issue on github if you think this is a bug.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|███████████████████████████████████████| 338M/338M [00:11<00:00, 31.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using default scheduler (no change)\n",
      "Starting LoRA application...\n",
      "Loading LoRA weights from: ./Wan2.1-T2V-1.3B-crush-smol-v0/\n",
      "LoRA weights loaded successfully.\n",
      "Enabling LoRA...\n",
      "LoRA enabled.\n",
      "Attempting to fuse LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA fused successfully.\n",
      "Attention slicing enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4915cefd4224e60a560bd6c940608c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[rawvideo @ 0x445cf140] Stream #0: not enough frames to estimate rate; consider increasing probesize\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://6d33f0eeb8c9428887.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themeeeeee = gr.themes.Base(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"gray\",\n",
    "    radius_size=gr.themes.Size(\n",
    "        xxs=\"6px\", xs=\"6px\", sm=\"8px\", md=\"10px\", lg=\"12px\", xl=\"14px\", xxl=\"16px\"\n",
    "    ),\n",
    "    spacing_size=gr.themes.Size(\n",
    "        xxs=\"2px\", xs=\"4px\", sm=\"6px\", md=\"10px\", lg=\"16px\", xl=\"24px\", xxl=\"32px\"\n",
    "    )\n",
    ").set(\n",
    "    body_background_fill=\"linear-gradient(135deg, #0f2027, #203a43, #2c5364)\",\n",
    "    body_text_color=\"white\",\n",
    "    block_background_fill=\"rgba(255, 255, 255, 0.08)\",\n",
    "    block_border_color=\"rgba(255, 255, 255, 0.2)\",\n",
    "    block_shadow=\"0 12px 40px rgba(0, 0, 0, 0.4)\",\n",
    "    input_background_fill=\"rgba(255, 255, 255, 0.1)\",\n",
    "    input_border_color=\"rgba(255, 255, 255, 0.2)\",\n",
    "    button_primary_background_fill=\"rgba(99, 102, 241, 0.85)\",\n",
    "    button_primary_text_color=\"white\",\n",
    "    button_primary_background_fill_hover=\"rgba(99, 102, 241, 1)\"\n",
    ")\n",
    "\n",
    "css_reset = \"\"\"\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap');\n",
    "\n",
    "body, #root, .gradio-container {\n",
    "    font-family: 'Inter', sans-serif !important;\n",
    "    background: linear-gradient(135deg, #0f2027, #203a43, #2c5364);\n",
    "    perspective: 1000px;\n",
    "    overflow-x: hidden;\n",
    "    animation: fadeIn 1s ease-in-out;\n",
    "    color: #00FFFF !important;\n",
    "    text-shadow: 0 0 5px #00FFFF, 0 0 10px #00FFFF !important;\n",
    "}\n",
    "\n",
    "#root {\n",
    "    transform-style: preserve-3d;\n",
    "}\n",
    "\n",
    ".gradio-container > * {\n",
    "   /* transform: rotateX(1deg) rotateY(-2deg); */\n",
    "    transition: transform 0.4s ease, box-shadow 0.4s ease;\n",
    "    backdrop-filter: blur(14px);\n",
    "    -webkit-backdrop-filter: blur(14px);\n",
    "    border-radius: 20px;\n",
    "    background: rgba(255, 255, 255, 0.05);\n",
    "    box-shadow: 0 12px 40px rgba(0, 0, 0, 0.3);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.15);\n",
    "    margin: 30px auto;\n",
    "    padding: 25px;\n",
    "    width: 85% !important;\n",
    "    box-sizing: border-box;\n",
    "    color: white !important;\n",
    "}\n",
    "\n",
    "/* Flashy animated text */\n",
    "@keyframes flashyColors {\n",
    "    0%   { color: #00FFFF; text-shadow: 0 0 10px #00FFFF; }  /* Cyan */\n",
    "    33%  { color: #66CCFF; text-shadow: 0 0 10px #66CCFF; }  /* Light Blue */\n",
    "    66%  { color: #00FF00; text-shadow: 0 0 10px #00FF00; }  /* Green */\n",
    "    100% { color: #00FFFF; text-shadow: 0 0 10px #00FFFF; }  /* Back to Cyan */\n",
    "}\n",
    "\n",
    "\n",
    "h1, h2, h3, h4, h5, p, span, label, button {\n",
    "    animation: flashyColors 15s infinite alternate !important;\n",
    "    text-shadow: 0 1px 1px #000, 0 2px 2px #00FFFF, 0 0 10px rgba(0,255,255,0.6);\n",
    "    font-weight: 600;\n",
    "}\n",
    "h1, h2, h3 {\n",
    "    text-align: center;\n",
    "    font-size: 2.2rem;\n",
    "}\n",
    "\n",
    "/* Buttons */\n",
    ".gr-button {\n",
    "    position: relative;\n",
    "    background: linear-gradient(to bottom, #00FFFF, #00BFFF) !important;\n",
    "    color: #000 !important;\n",
    "    font-weight: bold;\n",
    "    border-radius: 14px !important;\n",
    "    padding: 12px 20px;\n",
    "    border: none;\n",
    "    box-shadow: 0 4px 0 #009E9E, 0 6px 12px rgba(0, 255, 255, 0.4);\n",
    "    transition: transform 0.2s ease, box-shadow 0.2s ease;\n",
    "}\n",
    ".gr-button:hover {\n",
    "    transform: scale(1.05);\n",
    "    box-shadow: 0 6px 0 #008B8B, 0 10px 15px rgba(0, 255, 255, 0.5);\n",
    "}\n",
    ".gr-button:active {\n",
    "    transform: translateY(2px);\n",
    "    box-shadow: 0 2px 0 #007777, 0 6px 10px rgba(0, 255, 255, 0.3);\n",
    "}\n",
    ".gr-button.clicked::after {\n",
    "    content: '✅';\n",
    "    position: absolute;\n",
    "    right: 15px;\n",
    "    top: 50%;\n",
    "    transform: translateY(-50%);\n",
    "    font-size: 1.2rem;\n",
    "    animation: fadeOut 0.2s forwards;\n",
    "}\n",
    "@keyframes fadeOut {\n",
    "    0% { opacity: 1; }\n",
    "    80% { opacity: 0.5; }\n",
    "    100% { opacity: 0; content: ''; }\n",
    "}\n",
    "\n",
    "/* Form fields */\n",
    ".gr-textbox.gr-box, textarea, input {\n",
    "    background: rgba(40, 40, 40, 0.95) !important;\n",
    "    color: #FFFFFF !important;\n",
    "    border: 1px solid #00FFFF !important;\n",
    "    border-radius: 12px;\n",
    "    box-shadow: 0 0 10px rgba(0, 255, 255, 0.4);\n",
    "    animation: none !important;\n",
    "    text-shadow: none !important;\n",
    "}\n",
    "\n",
    "/* Slider */\n",
    ".gr-slider input[type=\"range\"] {\n",
    "    accent-color: #00FFFF !important;\n",
    "}\n",
    "\n",
    "/* Accordion */\n",
    ".gr-accordion {\n",
    "    background-color: #00ffff !important;\n",
    "    color: #ffffff !important;\n",
    "    border-radius: 8px;\n",
    "    border: 1px solid #444;\n",
    "    padding: 4px;\n",
    "}\n",
    ".gr-accordion .prose {\n",
    "    color: #1e1e2f !important;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    "/* Checkboxes */\n",
    "input[type=\"checkbox\"] {\n",
    "    appearance: none;\n",
    "    -webkit-appearance: none;\n",
    "    background-color: rgba(20, 20, 20, 0.8);\n",
    "    border: 2px solid #00FFFF;\n",
    "    border-radius: 6px;\n",
    "    width: 20px;\n",
    "    height: 20px;\n",
    "    cursor: pointer;\n",
    "    position: relative;\n",
    "    transition: all 0.2s ease-in-out;\n",
    "    box-shadow: 0 0 5px rgba(0, 255, 255, 0.4);\n",
    "    margin-right: 8px;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "\n",
    "input[type=\"checkbox\"]:checked {\n",
    "    background-color: #00FFFF;\n",
    "    box-shadow: 0 0 10px rgba(0, 255, 255, 0.8);\n",
    "}\n",
    "\n",
    "input[type=\"checkbox\"]::after {\n",
    "    content: '';\n",
    "    position: absolute;\n",
    "    top: 50%;\n",
    "    left: 50%;\n",
    "    transform: translate(-50%, -50%);\n",
    "    font-size: 14px;\n",
    "    color: #000;\n",
    "    font-weight: bold;\n",
    "    opacity: 0;\n",
    "    transition: opacity 0.2s ease-in-out;\n",
    "    animation: none !important;\n",
    "    text-shadow: none !important;\n",
    "}\n",
    "\n",
    "input[type=\"checkbox\"]:checked::after {\n",
    "    content: '✅';\n",
    "    color: #FF0000 !important;\n",
    "    opacity: 1;\n",
    "}\n",
    "\n",
    "input[type=\"radio\"] {\n",
    "    appearance: none;\n",
    "    -webkit-appearance: none;\n",
    "    border: 2px solid #ccc;\n",
    "    width: 20px;\n",
    "    height: 20px;\n",
    "    border-radius: 50%;\n",
    "    position: relative;\n",
    "    cursor: pointer;\n",
    "    outline: none;\n",
    "}\n",
    "\n",
    "/* Labels */\n",
    "label, .gr-radio-label span, .gr-check-label span {\n",
    "    font-weight: 700;\n",
    "}\n",
    "\n",
    "@keyframes fadeIn {\n",
    "    from { opacity: 0; transform: translateY(20px); }\n",
    "    to { opacity: 1; transform: translateY(0); }\n",
    "}\n",
    "\n",
    "#main-header h1 {\n",
    "    font-size: 3rem !important;\n",
    "    font-weight: 800;\n",
    "}\n",
    "\n",
    "#custom-radio input[type=\"radio\"] {\n",
    "    appearance: none;\n",
    "    -webkit-appearance: none;\n",
    "    width: 1em;\n",
    "    height: 1em;\n",
    "    margin-right: 10px;\n",
    "    border: 2px solid #ccc;\n",
    "    border-radius: 50%;\n",
    "    position: relative;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\n",
    "/* Show a custom emoji when selected */\n",
    "#custom-radio input[type=\"radio\"]:checked::before {\n",
    "    content: '✅';\n",
    "    position: absolute;\n",
    "    font-size: 1em;\n",
    "    top: -2px;\n",
    "    left: -2px;\n",
    "    color: red;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(theme=themeeeeee) as demo:\n",
    "    gr.HTML(css_reset)\n",
    "    gr.Markdown(\"# Optimized WAN 1.3B\", elem_id=\"main-header\")\n",
    "    gr.Markdown(\"<hr>\")\n",
    "\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\"## <div style='text-align:center; padding:15px;'>Metrics</div>\")\n",
    "        with gr.Row():\n",
    "            with gr.Accordion(\"Hardware\", open=True):\n",
    "                with gr.Row():\n",
    "                    mem_used = gr.Textbox(label=\"Memory used\", interactive=False)\n",
    "                    mem_free = gr.Textbox(label=\"Memory free\", interactive=False)\n",
    "                    gpu_util = gr.Textbox(label=\"GPU utilization\", interactive=False)\n",
    "                    temp = gr.Textbox(label=\"Temperature\", interactive=False)\n",
    "                    powe = gr.Textbox(label=\"Power draw\", interactive=False)\n",
    "                    ram_plot = gr.Plot(label=\"VRAM vs Time Graph\")\n",
    "            with gr.Accordion(\"Efficiency\", open=True):\n",
    "                with gr.Row():\n",
    "                    warmup_time_box = gr.Textbox(label=\"Warm Up Time\", interactive=False)\n",
    "                    clip_latency_box = gr.Textbox(label=\"Clip-wise latency\", interactive=False)\n",
    "                    frame_latency_box = gr.Textbox(label=\"Frame-wise latency\", interactive=False)\n",
    "                    throughput_box = gr.Textbox(label=\"Throughput\", interactive=False)\n",
    "                    dngraphoutput = gr.Plot(label=\"Time vs Denoising-Steps Graph\")\n",
    "        with gr.Row():\n",
    "            with gr.Accordion(\"Accuracy\", open=True):\n",
    "                with gr.Row():\n",
    "                    lpips_score_box = gr.Textbox(label=\"LPIPS score\", interactive=False)\n",
    "                    clip_score_box = gr.Textbox(label=\"CLIP score\", interactive=False)\n",
    "            with gr.Accordion(\"Others\", open=True):\n",
    "                with gr.Row():\n",
    "                    lolu = gr.Textbox(label=\"Load time\", interactive=False)\n",
    "    with gr.Row():\n",
    "        with gr.Group():\n",
    "            gr.Markdown(\"## <div style='text-align:center; padding:15px;'>Parameters</div>\")\n",
    "            res = gr.Radio(choices=[240, 480, 720], value=480, label=\"Output resolution\", interactive=True, elem_id=\"custom-radio\")\n",
    "            with gr.Row():\n",
    "                prompt = gr.Textbox(placeholder=\"e.g. A cat walking on moon\", label=\"Prompt\")\n",
    "                nprompt = gr.Textbox(value=negative_prompt, label=\"Negative prompt\")\n",
    "            with gr.Row(equal_height=True):\n",
    "                 fps = gr.Slider(minimum=1, maximum=120, label=\"FPS\", value=12)\n",
    "                 frames = gr.Slider(minimum=1, maximum=360, label=\"Number of frames\", value=60)\n",
    "            with gr.Row(equal_height=True):\n",
    "                 seed_slider = gr.Slider(-100, 100, value=42, step=1, label=\"Seed\", interactive=True)\n",
    "            with gr.Row(equal_height=True):\n",
    "                 guidance_slider = gr.Slider(4.0, 15.0, value=6.0, step=0.1, label=\"Guidance Scale\", interactive=True)\n",
    "                 steps_slider = gr.Slider(1, 200, value=50, step=1, label=\"Inference Steps\", interactive=True)\n",
    "\n",
    "            optimization_mode = gr.Radio(\n",
    "                label=\"Choose Optimization Mode\",\n",
    "                choices=[\"None\", \"Best\", \"Individual Techniques\"],\n",
    "                value=\"Best\",\n",
    "                interactive=True,\n",
    "                elem_id=\"custom-radio\"\n",
    "            )\n",
    "\n",
    "            individual_opts_row = gr.Row(visible=False)\n",
    "            Schedulers = gr.Radio(choices=['DPM (faster Speed)'], label=\"Diffusion Schedulers\", elem_id=\"custom-radio\")\n",
    "            to_load_model = gr.Checkbox(value=False, label=\"Reload the model\")\n",
    "            with individual_opts_row:\n",
    "                selected_techniques = gr.CheckboxGroup(\n",
    "                    label=\"Select Optimization Techniques\",\n",
    "                    choices=[\"Quantization\", \"LoRA\", \"Operator Fusion\", \"CPU offloading\", \"Attention Slicing\"],\n",
    "                )\n",
    "\n",
    "            def toggle_checkboxes(opt_mode):\n",
    "                return gr.update(visible=(\"Individual Techniques\" in opt_mode))\n",
    "\n",
    "            optimization_mode.change(\n",
    "                fn=toggle_checkboxes,\n",
    "                inputs=optimization_mode,\n",
    "                outputs=individual_opts_row\n",
    "            )\n",
    "\n",
    "        with gr.Group():\n",
    "            gr.Markdown(\"## <div style='text-align:center; padding:15px;'>Output</div>\")\n",
    "            output = gr.Video(label=\"Generated video\")\n",
    "\n",
    "    generate = gr.Button(\"Generate\")\n",
    "\n",
    "    # timers for GPU and load time monitoring\n",
    "    timer = gr.Timer()\n",
    "    timer.tick(fn=get_gpu_info_only, inputs=[], outputs=[mem_used, mem_free, gpu_util, temp, powe])\n",
    "\n",
    "    # wrapper with metrics and optimization techniques integration\n",
    "    def wrapper(prompt, nprompt, frames, fps, res, opt_mode, opt_tech, Schedulers, modell, guidance_scale, inference_steps, seed, progress=gr.Progress(track_tqdm=True)):\n",
    "    \n",
    "        global model_loaded\n",
    "        global pipe\n",
    "\n",
    "        data_type = None\n",
    "        pipe_quant_config = None\n",
    "\n",
    "        # start measuring GPU VRAM usage\n",
    "        monitor_thread = threading.Thread(target=monitor_vram)\n",
    "        monitor_thread.start()\n",
    "\n",
    "        opt = opt_mode\n",
    "\n",
    "        # give choice corresponding to technique\n",
    "        if \"Individual Techniques\" in opt_mode:\n",
    "            opt = opt_tech\n",
    "\n",
    "        # check if the model needs to be reloaded\n",
    "        if ((modell) or (model_loaded == False)):\n",
    "\n",
    "            print(\"Optimization mode:\", opt)\n",
    "\n",
    "            # apply quantization if it's in the selected list or \"Best\" mode\n",
    "            if ('Quantization' in opt or 'Best' in opt):\n",
    "                data_type, pipe_quant_config = apply_quantization()\n",
    "            else:\n",
    "                data_type = None\n",
    "                pipe_quant_config = None\n",
    "\n",
    "            # select the scheduler\n",
    "            if Schedulers == 'DPM (faster Speed)':\n",
    "                scheduler_ty = \"dpm\"\n",
    "            else:\n",
    "                scheduler_ty = \"default\"\n",
    "\n",
    "            to_offload = False\n",
    "\n",
    "            if ('CPU offloading' in opt):\n",
    "                to_offload = True\n",
    "\n",
    "            # load model\n",
    "            load_model(\n",
    "                data_type,\n",
    "                pipe_quant_config,\n",
    "                scheduler_type=scheduler_ty,\n",
    "                to_offload=to_offload\n",
    "            )\n",
    "\n",
    "            model_loaded = True\n",
    "\n",
    "        # apply techniques\n",
    "        if ('LoRA' in opt or 'Best' in opt):\n",
    "            apply_LoRA()\n",
    "        if ('Attention Slicing' in opt or 'Best' in opt):\n",
    "            apply_attention_slicing()\n",
    "        if ('Operator Fusion' in opt):\n",
    "            apply_operator_fusion()\n",
    "\n",
    "        video_path, clip_latency, frame_latency, throughputt, dngraph = generate_video(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=nprompt,\n",
    "            frames=frames,\n",
    "            fps=fps,\n",
    "            resolution=res,\n",
    "            seed=seed,\n",
    "            guidance_scale=guidance_scale,\n",
    "            inference_steps=inference_steps\n",
    "        )\n",
    "\n",
    "        extracted_frames = extract_frames_imageio(video_path)\n",
    "        lpips_score = compute_temporal_lpips(extracted_frames)\n",
    "\n",
    "        framess = extract_frames(num_frames=frames)\n",
    "        clipp_score = compute_clip_score(framess, prompt)\n",
    "\n",
    "        lpips_display = f\"{lpips_score:.3f}\" if not np.isnan(lpips_score) else \"N/A\"\n",
    "        clip_score_display = f\"{clipp_score:.3f}\"\n",
    "        warmup_time_display = f\"{warm_up_time:.3f} s\"\n",
    "        lolu_display = f\"{load_time:.3f} s\"\n",
    "\n",
    "        stop_event.set()\n",
    "        monitor_thread.join()\n",
    "\n",
    "        return video_path, clip_latency, frame_latency, lpips_display, clip_score_display, warmup_time_display, lolu_display, throughputt, dngraph,  gpu_usage_graph()\n",
    "    generate.click(\n",
    "        fn=wrapper,\n",
    "        inputs=[prompt, nprompt, frames, fps, res, optimization_mode, selected_techniques, Schedulers, to_load_model ,guidance_slider, steps_slider, seed_slider],\n",
    "        outputs=[output, clip_latency_box, frame_latency_box, lpips_score_box, clip_score_box, warmup_time_box, lolu, throughput_box, dngraphoutput,  ram_plot]\n",
    "    )\n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
