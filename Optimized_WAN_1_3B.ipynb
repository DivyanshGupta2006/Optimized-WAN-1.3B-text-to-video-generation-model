{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import lpips\n",
    "from torchvision import transforms\n",
    "import imageio\n",
    "import gradio as gr\n",
    "import torch\n",
    "import subprocess\n",
    "import clip\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables for accessing video and frames\n",
    "video_path = \"output_video.mp4\"\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\n",
    "output_frames_path = \"output_frames.mp4\"\n",
    "denoise_steps = [0] * 50\n",
    "load_time = 0\n",
    "warm_up_time = 0\n",
    "denoise_time = 0\n",
    "compile_time = 0\n",
    "is_LoRA_applied = False\n",
    "is_operator_fusion_applied = False\n",
    "is_quantization_applied = False\n",
    "is_CPU_offload_applied = False\n",
    "pipe = None\n",
    "model = None\n",
    "preprocess = None\n",
    "model_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the tqdm library to note time at each inference step\n",
    "original_tqdm = tqdm.auto.tqdm\n",
    "class TqdmSpy(original_tqdm):\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        global denoise_steps\n",
    "        denoise_steps[self.n-1] = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch modified tqdm to diffusers\n",
    "import diffusers\n",
    "diffusers.utils.tqdm = TqdmSpy\n",
    "tqdm.tqdm = TqdmSpy\n",
    "tqdm.auto.tqdm = TqdmSpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for extracting frames\n",
    "import decord\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "from decord import VideoReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA L4')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check device availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "# LPIPS loss function and transform definition\n",
    "loss_fn = lpips.LPIPS(net='vgg')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import (\n",
    "    DPMSolverMultistepScheduler,\n",
    "    DDIMScheduler,\n",
    "    LMSDiscreteScheduler\n",
    ")\n",
    "\n",
    "def set_diffusion_scheduler(pipe, scheduler_type):\n",
    "    config = pipe.scheduler.config\n",
    "\n",
    "    if scheduler_type == \"dpm\":\n",
    "        pipe.scheduler = DPMSolverMultistepScheduler.from_config(config)\n",
    "        print(\"[INFO] Scheduler set to DPM Solver Multistep\")\n",
    "    else:\n",
    "        print(\"[INFO] Using default scheduler (no change)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load model while recording time to load the model\n",
    "def load_model(data_type, pipeline_quant_config, scheduler_type, to_offload=True):\n",
    "    global load_time\n",
    "    global pipe\n",
    "    global model\n",
    "    global preprocess\n",
    "    pipe = None\n",
    "    model = None\n",
    "    preprocess = None\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    map = None\n",
    "\n",
    "    if to_offload:\n",
    "        print(\"Applying CPU offloading...Choosing device-map: balanced\")\n",
    "        map = \"balanced\"\n",
    "    \n",
    "    pipe = diffusers.DiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=map,\n",
    "        torch_dtype=data_type,\n",
    "        quantization_config=pipeline_quant_config\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # load model for CLIP Score\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    # Set scheduler\n",
    "    set_diffusion_scheduler(pipe, scheduler_type=scheduler_type)\n",
    "\n",
    "    patch_encode_prompt(pipe)\n",
    "\n",
    "    load_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for encoding prompt\n",
    "def patch_encode_prompt(pipe):\n",
    "    text_encoder_device = pipe.text_encoder.get_input_embeddings().weight.device\n",
    "    orig_encode_prompt = pipe.encode_prompt\n",
    "\n",
    "    def patched_encode_prompt(*args, **kwargs):\n",
    "        if \"device\" not in kwargs or kwargs[\"device\"] is None:\n",
    "            kwargs[\"device\"] = text_encoder_device\n",
    "        return orig_encode_prompt(*args, **kwargs)\n",
    "\n",
    "    pipe.encode_prompt = patched_encode_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for extracting frames from video for CLIP score\n",
    "def extract_frames(num_frames=8):\n",
    "    vr = VideoReader(video_path)\n",
    "    total_frames = len(vr)\n",
    "    indices = torch.linspace(0, total_frames - 1, steps=num_frames).long()\n",
    "    batch = vr.get_batch(indices).asnumpy()  # shape: (T, H, W, C)\n",
    "\n",
    "    return [Image.fromarray(frame) for frame in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for extracting frames from video for LPIPS score\n",
    "def extract_frames_imageio(video_path):\n",
    "    reader = imageio.get_reader(video_path)\n",
    "    frames = []\n",
    "    for frame in reader:\n",
    "        frames.append(Image.fromarray(frame))\n",
    "    reader.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to plot graph of denoising\n",
    "def denoise_graph():\n",
    "    global denoise_steps\n",
    "    global warm_up_time\n",
    "    global denoise_time\n",
    "    dns2 = [denoise_steps[i] - denoise_steps[i-1] for i in range(1, len(denoise_steps))]\n",
    "    denoise_time = sum(dns2)\n",
    "    plt.plot(list(range(len(dns2))), dns2)\n",
    "    plt.xlabel('Denoising step')\n",
    "    plt.ylabel('Time taken')\n",
    "    plt.title('Denoising')\n",
    "    graph = plt.gcf()\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for computing CLIP score\n",
    "def compute_clip_score(frames, text):\n",
    "    text_token = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_token).float()\n",
    "        frame_features = []\n",
    "        for frame in frames:\n",
    "            image_input = preprocess(frame).unsqueeze(0).to(device)\n",
    "            image_feature = model.encode_image(image_input).float()\n",
    "            frame_features.append(image_feature)\n",
    "        frame_features = torch.stack(frame_features).squeeze(1)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        frame_features = frame_features / frame_features.norm(dim=-1, keepdim=True)\n",
    "        similarities = (frame_features @ text_features.T).squeeze()\n",
    "        return similarities.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for computing LPIPS score\n",
    "def compute_temporal_lpips(frames):\n",
    "    if not frames or len(frames) < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(frames) - 1):\n",
    "        try:\n",
    "            img1 = transform(frames[i]).unsqueeze(0)\n",
    "            img2 = transform(frames[i + 1]).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dist = loss_fn(img1, img2)\n",
    "                score = dist.item()\n",
    "                scores.append(score)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if not scores:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to get GPU stats\n",
    "def get_gpu_stats():\n",
    "    try:\n",
    "        result = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=memory.used,memory.free,utilization.gpu,temperature.gpu,power.draw\",\n",
    "            \"--format=csv,nounits,noheader\"\n",
    "        ], encoding='utf-8')\n",
    "        memory_used, memory_free, utilization, temp, power = result.strip().split(', ')\n",
    "        return {\n",
    "            \"memory_used\": f\"{memory_used} MB\",\n",
    "            \"memory_free\": f\"{memory_free} MB\",\n",
    "            \"utilization\": f\"{utilization} %\",\n",
    "            \"temperature\": f\"{temp} Â°C\",\n",
    "            \"power\": f\"{power} W\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"memory_used\": \"Error\",\n",
    "            \"memory_free\": \"Error\",\n",
    "            \"utilization\": \"Error\",\n",
    "            \"temperature\": \"Error\",\n",
    "            \"power\": \"Error\"\n",
    "        }\n",
    "def get_gpu_info_only():\n",
    "    stats = get_gpu_stats()\n",
    "    return (\n",
    "        stats[\"memory_used\"],\n",
    "        stats[\"memory_free\"],\n",
    "        stats[\"utilization\"],\n",
    "        stats[\"temperature\"],\n",
    "        stats[\"power\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot GPU VRAM usage\n",
    "gpu_times=[]\n",
    "gpu_vram_usage=[]\n",
    "\n",
    "def only_vram():\n",
    "  statss = get_gpu_stats()\n",
    "  return(statss[\"memory_used\"])\n",
    "\n",
    "# Event to signal when to stop monitoring\n",
    "stop_event = threading.Event()\n",
    "stop_event.clear()\n",
    "\n",
    "def monitor_vram():\n",
    "    start = time.time()\n",
    "    while not stop_event.is_set():\n",
    "        mem = only_vram()\n",
    "        t = time.time() - start\n",
    "        if mem != \"Error\":\n",
    "            v = int(mem.split()[0])\n",
    "        else:\n",
    "            v = 0\n",
    "        gpu_times.append(t)\n",
    "        gpu_vram_usage.append(v)\n",
    "        time.sleep(0.25)\n",
    "def gpu_usage_graph():\n",
    "    global gpu_times\n",
    "    global gpu_vram_usage\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(gpu_times, gpu_vram_usage, marker='o', color='blue')\n",
    "    plt.xlabel('Time Elapsed (seconds)')\n",
    "    plt.ylabel('VRAM Used (MB)')\n",
    "    plt.title('GPU VRAM Usage During Video Generation')\n",
    "    plt.grid(True)\n",
    "    graph = plt.gcf()\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/async_helpers.py:129\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3254\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;66;03m# Store raw and processed history\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_history:\n\u001b[0;32m-> 3254\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m   3256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(cell, raw_cell)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/history.py:783\u001b[0m, in \u001b[0;36mHistoryManager.store_inputs\u001b[0;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_parsed\u001b[38;5;241m.\u001b[39mappend(source)\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_raw\u001b[38;5;241m.\u001b[39mappend(source_raw)\n\u001b[0;32m--> 783\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache_lock:\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache\u001b[38;5;241m.\u001b[39mappend((line_num, source, source_raw))\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Trigger to flush cache and write to DB.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# helper function to implement LoRA - loads weight from the git repo\n",
    "def apply_LoRA():\n",
    "    global is_LoRA_applied\n",
    "    if (is_LoRA_applied == False):\n",
    "      lora_path = \"./Wan2.1-T2V-1.3B-crush-smol-v0/\"\n",
    "      print(\"Starting LoRA application...\")\n",
    "\n",
    "      # Check if the path exists\n",
    "      if not os.path.exists(lora_path):\n",
    "          print(f\"LoRA path does not exist: {lora_path}\")\n",
    "          return\n",
    "\n",
    "      try:\n",
    "          print(f\"Loading LoRA weights from: {lora_path}\")\n",
    "          pipe.load_lora_weights(lora_path)\n",
    "          print(\"LoRA weights loaded successfully.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Error loading LoRA weights: {e}\")\n",
    "          return\n",
    "\n",
    "      try:\n",
    "          print(\"Enabling LoRA...\")\n",
    "          pipe.enable_lora()\n",
    "          print(\"LoRA enabled.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Error enabling LoRA: {e}\")\n",
    "          return\n",
    "\n",
    "      try:\n",
    "          print(\"Attempting to fuse LoRA...\")\n",
    "          pipe.fuse_lora()\n",
    "          is_LoRA_applied = True\n",
    "          print(\"LoRA fused successfully.\")\n",
    "      except Exception as e:\n",
    "          print(f\"LoRA fuse not supported: {e}\")\n",
    "    else:\n",
    "      print(\"LoRA already applied, so skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to implement operator fusion - covers model transformer and VAE\n",
    "def apply_operator_fusion():\n",
    "    global is_operator_fusion_applied\n",
    "    global compile_time\n",
    "    if (is_operator_fusion_applied == False):\n",
    "      startt = time.time()\n",
    "      print(\"Starting operator fusion...\")\n",
    "\n",
    "      try:\n",
    "          print(\"Compiling transformer...\")\n",
    "          pipe.transformer = torch.compile(pipe.transformer, mode=\"reduce-overhead\")\n",
    "          print(\"Transformer compiled successfully.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Error compiling transformer: {e}\")\n",
    "\n",
    "      try:\n",
    "          print(\"Compiling VAE...\")\n",
    "          pipe.vae = torch.compile(pipe.vae, mode=\"reduce-overhead\")\n",
    "          print(\"VAE compiled successfully.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Error compiling VAE: {e}\")\n",
    "      endd = time.time()\n",
    "      compile_time = endd - startt\n",
    "\n",
    "      is_operator_fusion_applied = True\n",
    "    else:\n",
    "      print(\"Operator Fusion already applied, so skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to apply quantization - 4 bit\n",
    "def apply_quantization():\n",
    "    print(\"Applying quantization configuration...\")\n",
    "\n",
    "    dtype = torch.float16\n",
    "    print(f\"Chosen dtype: {dtype}\")\n",
    "\n",
    "    quant_config = diffusers.quantizers.PipelineQuantizationConfig(\n",
    "        quant_backend=\"bitsandbytes_4bit\",\n",
    "        quant_kwargs={\n",
    "            \"load_in_4bit\": True,\n",
    "            \"bnb_4bit_quant_type\": \"nf4\",\n",
    "            \"bnb_4bit_compute_dtype\": torch.float16\n",
    "        },\n",
    "        components_to_quantize=[\"transformer\", \"text_encoder\", \"vae\"]\n",
    "    )\n",
    "\n",
    "    print(\"Quantization config created.\")\n",
    "    print(f\"Quant backend: {quant_config.quant_backend}\")\n",
    "    print(f\"Quant kwargs: {quant_config.quant_kwargs}\")\n",
    "    print(f\"Components to quantize: {quant_config.components_to_quantize}\")\n",
    "\n",
    "    return dtype, quant_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video(prompt, negative_prompt=\"Blurry, unrealistic, shaky\", frames=60, fps=12, resolution=480, inference_steps=50, guidance_scale=6.0, seed=42):\n",
    "\n",
    "    global denoise_steps\n",
    "    global warm_up_time\n",
    "    global denoise_time\n",
    "    \n",
    "    denoise_steps = [0] * inference_steps\n",
    "\n",
    "    # adjust frame count as per WAN's requirement\n",
    "    frames = 4 * frames + 1\n",
    "\n",
    "    # set height and width based on resolution\n",
    "    height = resolution\n",
    "    width = 832  # default\n",
    "\n",
    "    if height == 240:\n",
    "        width = 416\n",
    "    elif height == 720:\n",
    "        width = 1248\n",
    "    elif height == 1080:\n",
    "        width = 1872\n",
    "\n",
    "    # set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    # measure generation time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # generate video frames\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_frames=frames,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "        num_inference_steps=inference_steps\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    grph = denoise_graph()\n",
    "    total_latency = end_time - start_time\n",
    "    warm_up_time = total_latency - denoise_time\n",
    "    frame_latency = denoise_time / frames\n",
    "    Throughput = frames / total_latency\n",
    "\n",
    "    video = output.frames\n",
    "\n",
    "    if isinstance(video, np.ndarray):\n",
    "        video = np.squeeze(video)\n",
    "        video = (video * 255).clip(0, 255).astype(\"uint8\")\n",
    "    else:\n",
    "        raise TypeError(\"Unexpected output format from pipeline\")\n",
    "\n",
    "    # convert frames to PIL Images\n",
    "    frame_images = [Image.fromarray(frame) for frame in video]\n",
    "\n",
    "    # export videos\n",
    "    diffusers.utils.export_to_video(frame_images, video_path, fps=fps)\n",
    "    diffusers.utils.export_to_video(frame_images, output_frames_path, fps=1)\n",
    "\n",
    "    return video_path, f\"{total_latency:.3f} s\", f\"{frame_latency:.3f} s\", f\"{Throughput:.3f} fps\", grph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:302: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:302: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_1968/1392749088.py:302: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  return gr.update(visible=(opt_mode is \"Individual Techniques\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://071ac225f92499bd3d.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://071ac225f92499bd3d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization mode: Best\n",
      "Applying quantization configuration...\n",
      "Chosen dtype: torch.float16\n",
      "Quantization config created.\n",
      "Quant backend: bitsandbytes_4bit\n",
      "Quant kwargs: {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': torch.float16}\n",
      "Components to quantize: ['transformer', 'text_encoder', 'vae']\n",
      "Applying CPU offloading...Choosing device-map: balanced\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1f4a2f5704475ca749af10fe91de1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/400 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5987f3fbf8ce460fb50f1e9fcc473fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ec05ab0d244c4491d260bcf27530ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/751 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efaf3e681bf46cea197a0f23f5f1415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9412ce2756d4921afccb5a60b05d178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f610428818334b0aa9f60936b85d77a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9a6f3fcefa4dcd9368e5a8e74df9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf12b893e0a640729fd35e160298a7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/2.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5bfe52ee4a4976b6434fb0a0cfe04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20511d5b418c493db3e833f1e7700565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47373eadaa349e7a302631df4f31b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210c9f8482514595b91b966bbbf1d755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca04982806948de91127d0712b55e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d39d4c10b9438db4afd01cb0330eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1fc8a46b2c46cdb3f681cdea18bdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca11f1f2e6746a781372f0314c63f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)pytorch_model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e749c80f8084ea59e39e4dc472448fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)pytorch_model-00002-of-00002.safetensors:   0%|          | 0.00/677M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9f606666954749b2a0cfcb91c5a2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ion_pytorch_model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcee672f852b4460966c0320a6dff98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9bc3337def48e98c2bf596d3c12b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/508M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab07e77f9989424e825a208a79dcc88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are loading your model in 8bit or 4bit but no linear modules were found in your model. Please double check your model architecture, or submit an issue on github if you think this is a bug.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3df969cc6dd450ab9a99d79869a6b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1b1973a90842e8b8eab2b856cc6cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:07<00:00, 48.3MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Scheduler set to DPM Solver Multistep\n",
      "Starting LoRA application...\n",
      "LoRA path does not exist: ./Wan2.1-T2V-1.3B-crush-smol-v0/\n",
      "Starting operator fusion...\n",
      "Compiling transformer...\n",
      "Transformer compiled successfully.\n",
      "Compiling VAE...\n",
      "VAE compiled successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefc81925cc2451c9ac754f387a6784a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py:1917: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "W0728 21:18:47.564000 1968 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [1/0_1] Not enough SMs to use max_autotune_gemm mode\n",
      "skipping cudagraphs due to skipping cudagraphs due to cpu device (arg1_1). Found from : \n",
      "   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/diffusers/models/transformers/transformer_wan.py\", line 436, in forward\n",
      "    rotary_emb = self.rope(hidden_states)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/diffusers/models/transformers/transformer_wan.py\", line 207, in forward\n",
      "    freqs = self.freqs.to(hidden_states.device)\n",
      "\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0728 21:19:04.556000 1968 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0728 21:19:04.556000 1968 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8]    function: 'new_forward' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/accelerate/hooks.py:169)\n",
      "W0728 21:19:04.556000 1968 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/5: len(args) == 1                                         \n",
      "W0728 21:19:04.556000 1968 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0728 21:19:04.556000 1968 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA application...\n",
      "LoRA path does not exist: ./Wan2.1-T2V-1.3B-crush-smol-v0/\n",
      "Operator Fusion already applied, so skipping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5b408e2d0b405987f263143398cfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "themeeeeee = gr.themes.Base(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=\"gray\",\n",
    "    radius_size=gr.themes.Size(\n",
    "        xxs=\"6px\", xs=\"6px\", sm=\"8px\", md=\"10px\", lg=\"12px\", xl=\"14px\", xxl=\"16px\"\n",
    "    ),\n",
    "    spacing_size=gr.themes.Size(\n",
    "        xxs=\"2px\", xs=\"4px\", sm=\"6px\", md=\"10px\", lg=\"16px\", xl=\"24px\", xxl=\"32px\"\n",
    "    )\n",
    ").set(\n",
    "    body_background_fill=\"linear-gradient(135deg, #0f2027, #203a43, #2c5364)\",\n",
    "    body_text_color=\"white\",\n",
    "    block_background_fill=\"rgba(255, 255, 255, 0.08)\",\n",
    "    block_border_color=\"rgba(255, 255, 255, 0.2)\",\n",
    "    block_shadow=\"0 12px 40px rgba(0, 0, 0, 0.4)\",\n",
    "    input_background_fill=\"rgba(255, 255, 255, 0.1)\",\n",
    "    input_border_color=\"rgba(255, 255, 255, 0.2)\",\n",
    "    button_primary_background_fill=\"rgba(99, 102, 241, 0.85)\",\n",
    "    button_primary_text_color=\"white\",\n",
    "    button_primary_background_fill_hover=\"rgba(99, 102, 241, 1)\"\n",
    ")\n",
    "\n",
    "css_reset = \"\"\"\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap');\n",
    "\n",
    "body, #root, .gradio-container {\n",
    "    font-family: 'Inter', sans-serif !important;\n",
    "    background: linear-gradient(135deg, #0f2027, #203a43, #2c5364);\n",
    "    perspective: 1000px;\n",
    "    overflow-x: hidden;\n",
    "    animation: fadeIn 1s ease-in-out;\n",
    "    color: #00FFFF !important;\n",
    "    text-shadow: 0 0 5px #00FFFF, 0 0 10px #00FFFF !important;\n",
    "}\n",
    "\n",
    "#root {\n",
    "    transform-style: preserve-3d;\n",
    "}\n",
    "\n",
    ".gradio-container > * {\n",
    "   /* transform: rotateX(1deg) rotateY(-2deg); */\n",
    "    transition: transform 0.4s ease, box-shadow 0.4s ease;\n",
    "    backdrop-filter: blur(14px);\n",
    "    -webkit-backdrop-filter: blur(14px);\n",
    "    border-radius: 20px;\n",
    "    background: rgba(255, 255, 255, 0.05);\n",
    "    box-shadow: 0 12px 40px rgba(0, 0, 0, 0.3);\n",
    "    border: 1px solid rgba(255, 255, 255, 0.15);\n",
    "    margin: 30px auto;\n",
    "    padding: 25px;\n",
    "    width: 85% !important;\n",
    "    box-sizing: border-box;\n",
    "    color: white !important;\n",
    "}\n",
    "\n",
    "/* Flashy animated text */\n",
    "@keyframes flashyColors {\n",
    "    0%   { color: #00FFFF; text-shadow: 0 0 10px #00FFFF; }  /* Cyan */\n",
    "    33%  { color: #66CCFF; text-shadow: 0 0 10px #66CCFF; }  /* Light Blue */\n",
    "    66%  { color: #00FF00; text-shadow: 0 0 10px #00FF00; }  /* Green */\n",
    "    100% { color: #00FFFF; text-shadow: 0 0 10px #00FFFF; }  /* Back to Cyan */\n",
    "}\n",
    "\n",
    "\n",
    "h1, h2, h3, h4, h5, p, span, label, button {\n",
    "    animation: flashyColors 15s infinite alternate !important;\n",
    "    text-shadow: 0 1px 1px #000, 0 2px 2px #00FFFF, 0 0 10px rgba(0,255,255,0.6);\n",
    "    font-weight: 600;\n",
    "}\n",
    "h1, h2, h3 {\n",
    "    text-align: center;\n",
    "    font-size: 2.2rem;\n",
    "}\n",
    "\n",
    "/* Buttons */\n",
    ".gr-button {\n",
    "    position: relative;\n",
    "    background: linear-gradient(to bottom, #00FFFF, #00BFFF) !important;\n",
    "    color: #000 !important;\n",
    "    font-weight: bold;\n",
    "    border-radius: 14px !important;\n",
    "    padding: 12px 20px;\n",
    "    border: none;\n",
    "    box-shadow: 0 4px 0 #009E9E, 0 6px 12px rgba(0, 255, 255, 0.4);\n",
    "    transition: transform 0.2s ease, box-shadow 0.2s ease;\n",
    "}\n",
    ".gr-button:hover {\n",
    "    transform: scale(1.05);\n",
    "    box-shadow: 0 6px 0 #008B8B, 0 10px 15px rgba(0, 255, 255, 0.5);\n",
    "}\n",
    ".gr-button:active {\n",
    "    transform: translateY(2px);\n",
    "    box-shadow: 0 2px 0 #007777, 0 6px 10px rgba(0, 255, 255, 0.3);\n",
    "}\n",
    ".gr-button.clicked::after {\n",
    "    content: 'ðŸ”˜';\n",
    "    position: absolute;\n",
    "    right: 15px;\n",
    "    top: 50%;\n",
    "    transform: translateY(-50%);\n",
    "    font-size: 1.2rem;\n",
    "    animation: fadeOut 0.2s forwards;\n",
    "}\n",
    "@keyframes fadeOut {\n",
    "    0% { opacity: 1; }\n",
    "    80% { opacity: 0.5; }\n",
    "    100% { opacity: 0; content: ''; }\n",
    "}\n",
    "\n",
    "/* Form fields */\n",
    ".gr-textbox.gr-box, textarea, input {\n",
    "    background: rgba(40, 40, 40, 0.95) !important;\n",
    "    color: #FFFFFF !important;\n",
    "    border: 1px solid #00FFFF !important;\n",
    "    border-radius: 12px;\n",
    "    box-shadow: 0 0 10px rgba(0, 255, 255, 0.4);\n",
    "    animation: none !important;\n",
    "    text-shadow: none !important;\n",
    "}\n",
    "\n",
    "/* Slider */\n",
    ".gr-slider input[type=\"range\"] {\n",
    "    accent-color: #00FFFF !important;\n",
    "}\n",
    "\n",
    "/* Accordion */\n",
    ".gr-accordion {\n",
    "    background-color: #00ffff !important;\n",
    "    color: #ffffff !important;\n",
    "    border-radius: 8px;\n",
    "    border: 1px solid #444;\n",
    "    padding: 4px;\n",
    "}\n",
    ".gr-accordion .prose {\n",
    "    color: #1e1e2f !important;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    "/* Checkboxes */\n",
    "input[type=\"checkbox\"] {\n",
    "    appearance: none;\n",
    "    -webkit-appearance: none;\n",
    "    background-color: rgba(20, 20, 20, 0.8);\n",
    "    border: 2px solid #00FFFF;\n",
    "    border-radius: 6px;\n",
    "    width: 20px;\n",
    "    height: 20px;\n",
    "    cursor: pointer;\n",
    "    position: relative;\n",
    "    transition: all 0.2s ease-in-out;\n",
    "    box-shadow: 0 0 5px rgba(0, 255, 255, 0.4);\n",
    "    margin-right: 8px;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "\n",
    "input[type=\"checkbox\"]:checked {\n",
    "    background-color: #00FFFF;\n",
    "    box-shadow: 0 0 10px rgba(0, 255, 255, 0.8);\n",
    "}\n",
    "\n",
    "input[type=\"checkbox\"]::after {\n",
    "    content: '';\n",
    "    position: absolute;\n",
    "    top: 50%;\n",
    "    left: 50%;\n",
    "    transform: translate(-50%, -50%);\n",
    "    font-size: 14px;\n",
    "    color: #000;\n",
    "    font-weight: bold;\n",
    "    opacity: 0;\n",
    "    transition: opacity 0.2s ease-in-out;\n",
    "    animation: none !important;\n",
    "    text-shadow: none !important;\n",
    "}\n",
    "\n",
    "input[type=\"checkbox\"]:checked::after {\n",
    "    content: 'ðŸ”˜';\n",
    "    color: #FF0000 !important;\n",
    "    opacity: 1;\n",
    "}\n",
    "\n",
    "input[type=\"radio\"] {\n",
    "    appearance: none;\n",
    "    -webkit-appearance: none;\n",
    "    border: 2px solid #ccc;\n",
    "    width: 20px;\n",
    "    height: 20px;\n",
    "    border-radius: 50%;\n",
    "    position: relative;\n",
    "    cursor: pointer;\n",
    "    outline: none;\n",
    "}\n",
    "\n",
    "/* Labels */\n",
    "label, .gr-radio-label span, .gr-check-label span {\n",
    "    font-weight: 700;\n",
    "}\n",
    "\n",
    "@keyframes fadeIn {\n",
    "    from { opacity: 0; transform: translateY(20px); }\n",
    "    to { opacity: 1; transform: translateY(0); }\n",
    "}\n",
    "\n",
    "#main-header h1 {\n",
    "    font-size: 3rem !important;\n",
    "    font-weight: 800;\n",
    "}\n",
    "\n",
    "#custom-radio input[type=\"radio\"] {\n",
    "    appearance: none;\n",
    "    -webkit-appearance: none;\n",
    "    width: 1em;\n",
    "    height: 1em;\n",
    "    margin-right: 10px;\n",
    "    border: 2px solid #ccc;\n",
    "    border-radius: 50%;\n",
    "    position: relative;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\n",
    "/* Show a custom emoji when selected */\n",
    "#custom-radio input[type=\"radio\"]:checked::before {\n",
    "    content: 'ðŸ”˜';\n",
    "    position: absolute;\n",
    "    font-size: 1em;\n",
    "    top: -2px;\n",
    "    left: -2px;\n",
    "    color: red;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(theme=themeeeeee) as demo:\n",
    "    gr.HTML(css_reset)\n",
    "    gr.Markdown(\"# Perfect Chinmayee Shambhavi Wifey Ashika\", elem_id=\"main-header\")\n",
    "    gr.Markdown(\"<hr>\")\n",
    "\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\"## <div style='text-align:center; padding:15px;'>Metrics</div>\")\n",
    "        with gr.Row():\n",
    "            with gr.Accordion(\"Hardware\", open=True):\n",
    "                with gr.Row():\n",
    "                    mem_used = gr.Textbox(label=\"Memory used\", interactive=False)\n",
    "                    mem_free = gr.Textbox(label=\"Memory free\", interactive=False)\n",
    "                    gpu_util = gr.Textbox(label=\"GPU utilization\", interactive=False)\n",
    "                    temp = gr.Textbox(label=\"Temperature\", interactive=False)\n",
    "                    powe = gr.Textbox(label=\"Power draw\", interactive=False)\n",
    "                    ram_plot = gr.Plot(label=\"VRAM vs Time Graph\")\n",
    "            with gr.Accordion(\"Efficiency\", open=True):\n",
    "                with gr.Row():\n",
    "                    warmup_time_box = gr.Textbox(label=\"Warm Up Time\", interactive=False)\n",
    "                    clip_latency_box = gr.Textbox(label=\"Clip-wise latency\", interactive=False)\n",
    "                    frame_latency_box = gr.Textbox(label=\"Frame-wise latency\", interactive=False)\n",
    "                    throughput_box = gr.Textbox(label=\"Throughput\", interactive=False)\n",
    "                    dngraphoutput = gr.Plot(label=\"Time vs Denoising-Steps Graph\")\n",
    "        with gr.Row():\n",
    "            with gr.Accordion(\"Accuracy\", open=True):\n",
    "                with gr.Row():\n",
    "                    lpips_score_box = gr.Textbox(label=\"LPIPS score\", interactive=False)\n",
    "                    clip_score_box = gr.Textbox(label=\"CLIP score\", interactive=False)\n",
    "            with gr.Accordion(\"Others\", open=True):\n",
    "                with gr.Row():\n",
    "                    compile_time_box = gr.Textbox(label=\"Compile time\", interactive=False)\n",
    "                    lolu = gr.Textbox(label=\"Load time\", interactive=False)\n",
    "    with gr.Row():\n",
    "        with gr.Group():\n",
    "            gr.Markdown(\"## <div style='text-align:center; padding:15px;'>Parameters</div>\")\n",
    "            res = gr.Radio(choices=[240, 480, 720, 1080], value=480, label=\"Output resolution\", interactive=True, elem_id=\"custom-radio\")\n",
    "            with gr.Row():\n",
    "                prompt = gr.Textbox(placeholder=\"e.g. A cat walking on moon\", label=\"Prompt\")\n",
    "                nprompt = gr.Textbox(value=negative_prompt, label=\"Negative prompt\")\n",
    "            with gr.Row(equal_height=True):\n",
    "                 fps = gr.Slider(minimum=1, maximum=120, label=\"FPS\", value=12)\n",
    "                 frames = gr.Slider(minimum=1, maximum=360, label=\"Number of frames\", value=60)\n",
    "            with gr.Row(equal_height=True):\n",
    "                 seed_slider = gr.Slider(0, 100, value=42, step=1, label=\"Seed\", interactive=True)\n",
    "            with gr.Row(equal_height=True):\n",
    "                 guidance_slider = gr.Slider(4.0, 15.0, value=6.0, step=0.1, label=\"Guidance Scale\", interactive=True)\n",
    "                 steps_slider = gr.Slider(1, 100, value=50, step=1, label=\"Inference Steps\", interactive=True)\n",
    "\n",
    "            optimization_mode = gr.Radio(\n",
    "                label=\"Choose Optimization Mode\",\n",
    "                choices=[\"None\", \"Best\", \"Individual Techniques\"],\n",
    "                value=\"Best\",\n",
    "                interactive=True,\n",
    "                elem_id=\"custom-radio\"\n",
    "            )\n",
    "\n",
    "            individual_opts_row = gr.Row(visible=False)\n",
    "            Schedulers = gr.Radio(choices=['DPM (faster Speed)'], label=\"Diffusion Schedulers\", elem_id=\"custom-radio\")\n",
    "            to_load_model = gr.Checkbox(value=False, label=\"Reload the model\")\n",
    "            with individual_opts_row:\n",
    "                selected_techniques = gr.CheckboxGroup(\n",
    "                    label=\"Select Optimization Techniques\",\n",
    "                    choices=[\"Quantization\", \"LoRA\", \"Operator Fusion\", \"CPU offloading\"],\n",
    "                )\n",
    "\n",
    "            def toggle_checkboxes(opt_mode):\n",
    "                return gr.update(visible=(opt_mode is \"Individual Techniques\"))\n",
    "\n",
    "            optimization_mode.change(\n",
    "                fn=toggle_checkboxes,\n",
    "                inputs=optimization_mode,\n",
    "                outputs=individual_opts_row\n",
    "            )\n",
    "\n",
    "        with gr.Group():\n",
    "            gr.Markdown(\"## <div style='text-align:center; padding:15px;'>Output</div>\")\n",
    "            output = gr.Video(label=\"Generated video\")\n",
    "\n",
    "    generate = gr.Button(\"Generate\")\n",
    "\n",
    "    # timers for GPU and load time monitoring\n",
    "    timer = gr.Timer()\n",
    "    timer.tick(fn=get_gpu_info_only, inputs=[], outputs=[mem_used, mem_free, gpu_util, temp, powe])\n",
    "\n",
    "    # wrapper with metrics and optimization techniques integration\n",
    "    def wrapper(prompt, nprompt, frames, fps, res, opt_mode, opt_tech, Schedulers, modell, guidance_scale, inference_steps, seed, progress=gr.Progress()):\n",
    "    \n",
    "        global compile_time\n",
    "        global model_loaded\n",
    "        global pipe\n",
    "\n",
    "        data_type = None\n",
    "        pipe_quant_config = None\n",
    "\n",
    "        # start measuring GPU VRAM usage\n",
    "        monitor_thread = threading.Thread(target=monitor_vram)\n",
    "        monitor_thread.start()\n",
    "\n",
    "        opt = opt_mode\n",
    "\n",
    "        # give choice corresponding to technique\n",
    "        if \"Individual Techniques\" in opt_mode:\n",
    "            opt = opt_tech\n",
    "\n",
    "        # check if the model needs to be reloaded\n",
    "        if ((modell) or (model_loaded == False)):\n",
    "\n",
    "            print(\"Optimization mode:\", opt)\n",
    "\n",
    "            # apply quantization if it's in the selected list or \"Best\" mode\n",
    "            if ('Quantization' in opt or 'Best' in opt):\n",
    "                data_type, pipe_quant_config = apply_quantization()\n",
    "            else:\n",
    "                data_type = None\n",
    "                pipe_quant_config = None\n",
    "\n",
    "            # select the scheduler\n",
    "            if Schedulers == 'DPM (faster Speed)':\n",
    "                scheduler_ty = \"dpm\"\n",
    "            else:\n",
    "                scheduler_ty = \"default\"\n",
    "\n",
    "            to_offload = False\n",
    "\n",
    "            if ('CPU offloading' in opt or 'Best' in opt):\n",
    "                to_offload = True\n",
    "\n",
    "            # load model\n",
    "            load_model(\n",
    "                data_type,\n",
    "                pipe_quant_config,\n",
    "                scheduler_type=scheduler_ty,\n",
    "                to_offload=to_offload\n",
    "            )\n",
    "\n",
    "            model_loaded = True\n",
    "\n",
    "        # apply techniques\n",
    "        if ('LoRA' in opt or 'Best' in opt):\n",
    "            apply_LoRA()\n",
    "        if ('Operator Fusion' in opt or 'Best' in opt):\n",
    "            apply_operator_fusion()\n",
    "\n",
    "        video_path, clip_latency, frame_latency, throughputt, dngraph = generate_video(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=nprompt,\n",
    "            frames=frames,\n",
    "            fps=fps,\n",
    "            resolution=res,\n",
    "            seed=seed,\n",
    "            guidance_scale=guidance_scale,\n",
    "            inference_steps=inference_steps\n",
    "        )\n",
    "\n",
    "        extracted_frames = extract_frames_imageio(video_path)\n",
    "        lpips_score = compute_temporal_lpips(extracted_frames)\n",
    "\n",
    "        framess = extract_frames(num_frames=frames)\n",
    "        clipp_score = compute_clip_score(framess, prompt)\n",
    "\n",
    "        lpips_display = f\"{lpips_score:.3f}\" if not np.isnan(lpips_score) else \"N/A\"\n",
    "        clip_score_display = f\"{clipp_score:.3f}\"\n",
    "        warmup_time_display = f\"{warm_up_time:.3f} s\"\n",
    "        lolu_display = f\"{load_time:.3f} s\"\n",
    "        compile_time_display = f\"{compile_time:.3f} s\"\n",
    "\n",
    "        stop_event.set()\n",
    "        monitor_thread.join()\n",
    "\n",
    "        return video_path, clip_latency, frame_latency, lpips_display, clip_score_display, warmup_time_display, lolu_display, throughputt, dngraph, compile_time_display, gpu_usage_graph()\n",
    "    generate.click(\n",
    "        fn=wrapper,\n",
    "        inputs=[prompt, nprompt, frames, fps, res, optimization_mode, selected_techniques, Schedulers, to_load_model ,guidance_slider, steps_slider, seed_slider],\n",
    "        outputs=[output, clip_latency_box, frame_latency_box, lpips_score_box, clip_score_box, warmup_time_box, lolu, throughput_box, dngraphoutput, compile_time_box, ram_plot]\n",
    "    )\n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
